services:
  # PostgreSQL database
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: yourpassword
      POSTGRES_DB: postgres
    # volumes:
    #   - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Recipe app 
  recipe_app:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    environment:
      - DB_HOST=postgres
      - DB_USER=postgres
      - DB_PASSWORD=yourpassword
      - DB_NAME=postgres
      - LLM_SERVICE_URL=http://ollama:11434
      - LLM_MODEL=llama-2-7b-chat-q4 # Switch to a larger model for better resource utilization
      - OLLAMA_MODEL=llama-2-7b-chat-q4
      - REQUEST_TIMEOUT=120000 # Increase timeout to 120 seconds
    ports:
      - "3000:3000"
    # volumes:
    #   # Mount everything except node_modules and use a named volume for node_modules
    #   - ./src:/app/src
    #   - ./package.json:/app/package.json
    #   - ./package-lock.json:/app/package-lock.json
    #   - ./tsconfig.json:/app/tsconfig.json
    #   - recipe_app_node_modules:/app/node_modules

  # Liquibase for database migrations
  liquibase:
    image: liquibase/liquibase:4.31.1
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - .:/liquibase/changelog
    command: >
      --changelog-file=liquibase-changelog.xml
      --url=jdbc:postgresql://postgres:5432/postgres
      --username=postgres
      --password=yourpassword
      --search-path=/liquibase/changelog/src/db/migrations
      update

  # Ollama with memory-efficient configuration
  ollama:
    image: ollama/ollama:latest  # This image supports ARM64/Apple Silicon natively
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: always
    # Adjust resource allocation to ensure we don't run out of memory
    deploy:
      resources:
        limits:
          cpus: '10' # Increase CPU allocation to 10
          memory: 64G # Increase memory allocation to 64GB
        reservations:
          cpus: '10' # Reserve 10 CPUs
          memory: 64G # Reserve 64GB
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
      # Add memory-specific settings to optimize for limited memory
      - OLLAMA_MODEL_PATH=/root/.ollama/models
      - OLLAMA_THREADS=8 # Reduce threads for better stability
      - OLLAMA_PARALLEL=4 # Reduce parallelism to improve stability
      - OLLAMA_BATCH_SIZE=512 # Smaller batch size for more reliable processing
      - OLLAMA_CONTEXT_SIZE=4096 # Keep context size consistent
      - OLLAMA_MEMORY_LIMIT=24G # Lower memory limit for better system stability
      - OLLAMA_EMBEDDING_ASI=true # Enable alignment scanning for better performance
      - OLLAMA_USE_MLOCK=true # Prevent memory swapping
      - OLLAMA_NUM_GPU=0 # CPU-only mode for better compatibility

    entrypoint: >
      sh -c "
        # Start ollama service
        ollama serve &
        
        # Wait for ollama to be ready
        sleep 15
        
        # Check if the model manifest exists before pulling
        if [ ! -f \"$OLLAMA_MODEL_PATH/llama-2-7b-chat-q4/manifest.json\" ]; then
          echo 'Model manifest not found. Attempting to pull the model...'
          ollama pull llama-2-7b-chat-q4 || echo 'Failed to pull the model. Please verify the model name and path.'
        else
          echo 'Model manifest found. Skipping pull.'
        fi
        
        # Keep container running
        wait
      "

volumes:
  # postgres_data:
  ollama_data:
  # recipe_app_node_modules: